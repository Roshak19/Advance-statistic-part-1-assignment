{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the properties of the F-distribution.**\n",
        "\n"
      ],
      "metadata": {
        "id": "31Mdn-Nz19mq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Properties of the F-distribution**\n",
        "\n",
        "1. **Right-Skewed**:\n",
        "   - The F-distribution is **positively skewed**, meaning it has a longer tail on the right. It starts at 0 and extends to infinity.\n",
        "\n",
        "2. **Non-negative**:\n",
        "   - F-values are always **non-negative** (≥ 0) because they represent a ratio of variances, which cannot be negative.\n",
        "\n",
        "3. **Depends on Degrees of Freedom**:\n",
        "   - The shape of the F-distribution is determined by **two degrees of freedom**: one for the **numerator** (between-group variance) and one for the **denominator** (within-group variance).\n",
        "   - As the degrees of freedom increase, the distribution becomes less skewed and approaches a normal distribution.\n",
        "\n",
        "4. **Used for Comparing Variances**:\n",
        "   - The F-distribution is used in tests like **ANOVA** to compare variances between two or more groups.\n",
        "\n",
        "5. **Hypothesis Testing**:\n",
        "   - The F-statistic compares the variances of groups. A large F-statistic suggests significant differences, while a small F-statistic suggests no difference.\n",
        "\n",
        "6. **Critical Values and P-values**:\n",
        "   - The F-distribution is used to calculate **p-values** and **critical values** in hypothesis tests. If the computed F-statistic exceeds the critical value, we reject the null hypothesis.\n",
        "   ---\n",
        "\n"
      ],
      "metadata": {
        "id": "jal91yA32vqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**"
      ],
      "metadata": {
        "id": "s9vPvOT43FLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The **F-distribution** is used in the following statistical tests:\n",
        "\n",
        "1. **Analysis of Variance (ANOVA)**: To compare means across multiple groups by testing the ratio of between-group variance to within-group variance. The F-distribution is appropriate because it models the ratio of variances.\n",
        "\n",
        "2. **F-test for Comparing Two Variances**: To test if two population variances are equal. The F-statistic is the ratio of the two sample variances and follows an F-distribution under the null hypothesis.\n",
        "\n",
        "3. **Regression Analysis (F-test for Model Significance)**: To assess the overall significance of a regression model, by comparing the explained variance to the unexplained variance. The F-distribution is used because it models the ratio of mean squares in regression.\n",
        "\n",
        "4. **Test for Nested Models**: To compare two nested models (one simpler than the other) by testing the ratio of their residual variances. The F-distribution is used because it models the ratio of variances between the models.\n",
        "\n",
        "### Why It's Appropriate:\n",
        "The F-distribution is used in these tests because it describes the ratio of two variances (or mean squares), and these tests are concerned with comparing variances, which is exactly what the F-distribution models.\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TarrzXJ63g6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?**"
      ],
      "metadata": {
        "id": "R_N4z5aJ31y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For an **F-test** to compare the variances of two populations, the following key assumptions must be met:\n",
        "\n",
        "1. **Independence**: The two samples must be independent of each other.\n",
        "\n",
        "2. **Normality**: Both populations (or the samples) should follow a **normal distribution**. This assumption is important because the F-distribution arises from the ratio of two chi-squared distributions, which are based on normality.\n",
        "\n",
        "3. **Homogeneity of Variances**: The two populations being compared should have **equal variances** under the null hypothesis.\n",
        "\n",
        "These assumptions ensure the validity of the F-test and the accuracy of the results."
      ],
      "metadata": {
        "id": "fnM_AFd038vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What is the purpose of ANOVA, and how does it differ from a t-test?**"
      ],
      "metadata": {
        "id": "7VQKn86P4MCL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Purpose of **ANOVA**:\n",
        "**ANOVA** (Analysis of Variance) is a statistical technique used to compare the means of **three or more groups** to determine if there is a significant difference among them. It tests the null hypothesis that all group means are equal. ANOVA works by analyzing the variance within each group and between the groups.\n",
        "\n",
        "### How ANOVA Differs from a **t-test**:\n",
        "1. **Number of Groups**:\n",
        "   - **T-test**: Compares the means of **two groups** to see if they are significantly different from each other.\n",
        "   - **ANOVA**: Compares the means of **three or more groups** to check if at least one group mean differs significantly from the others.\n",
        "\n",
        "2. **Hypothesis Testing**:\n",
        "   - **T-test**: Tests whether the difference between two group means is statistically significant.\n",
        "   - **ANOVA**: Tests whether there are any significant differences **among the means of multiple groups**. If ANOVA indicates a significant difference, follow-up tests (like Tukey's HSD) are used to identify which specific groups are different.\n",
        "\n",
        "3. **Type of Test**:\n",
        "   - **T-test**: Relies on comparing the difference in means and the standard error of the difference for two groups.\n",
        "   - **ANOVA**: Uses variance analysis by comparing the variability between groups (between-group variance) to the variability within groups (within-group variance).\n",
        "\n",
        "### Key Difference:\n",
        "While the **t-test** is limited to comparing two groups, **ANOVA** extends the comparison to three or more groups, making it more versatile for situations involving multiple groups.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jGoTq0Sg4TQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups.**"
      ],
      "metadata": {
        "id": "C6ROgwln4j8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### When to Use **One-Way ANOVA** Instead of Multiple **t-tests**:\n",
        "\n",
        "Use a **One-Way ANOVA** when you are comparing the means of **three or more groups** and want to test if at least one of the group means is significantly different from the others.\n",
        "\n",
        "### Why One-Way ANOVA is Preferred Over Multiple t-tests:\n",
        "\n",
        "1. **Control of Type I Error**:\n",
        "   - When you conduct multiple **t-tests**, each test carries a chance of making a **Type I error** (incorrectly rejecting the null hypothesis). The more t-tests you run, the higher the cumulative probability of a false positive.\n",
        "   - **One-Way ANOVA** controls the **overall Type I error rate** by testing all group means simultaneously. It evaluates the variance between all groups in one test, reducing the chance of making an error by chance.\n",
        "\n",
        "2. **Efficiency**:\n",
        "   - **ANOVA** tests all group comparisons in one analysis, while **multiple t-tests** require separate comparisons between pairs of groups. This makes **ANOVA** more efficient, particularly with large numbers of groups.\n",
        "\n",
        "3. **Comprehensive Testing**:\n",
        "   - **One-Way ANOVA** tests if there are any significant differences **among all groups**. Multiple t-tests only test specific pairs of groups and cannot tell you if there is an overall difference across all groups.\n",
        "   - **ANOVA** provides a single test statistic (F-statistic) to assess the overall difference, and if significant, follow-up tests (e.g., Tukey's HSD) can identify which specific groups differ.\n",
        "\n",
        "### Summary:\n",
        "**One-Way ANOVA** is preferred when comparing three or more groups because it reduces the risk of Type I error, is more efficient, and provides a comprehensive analysis of group differences in one step, while multiple t-tests are more error-prone and less efficient for multiple comparisons.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Od87jTpa49BH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?**"
      ],
      "metadata": {
        "id": "P-s5SPXx5Hz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In **ANOVA (Analysis of Variance)**, the total variance in the data is divided into two parts:\n",
        "\n",
        "1. **Between-Group Variance**: This reflects the variability caused by differences between the **group means**. If the group means are widely spread out, the between-group variance will be large, suggesting that the groups differ significantly from one another.\n",
        "\n",
        "2. **Within-Group Variance**: This represents the variability within each group, or how individual data points vary from their own group mean. High within-group variance indicates more variation within each group, often due to random factors or inherent variability in the data.\n",
        "\n",
        "### Contribution to the **F-statistic**:\n",
        "The **F-statistic** is the ratio of **between-group variance** to **within-group variance**:\n",
        "\n",
        "- A **large F-statistic** (high between-group variance relative to within-group variance) indicates that the group means differ more than would be expected by random chance, suggesting a significant effect.\n",
        "- A **small F-statistic** (low between-group variance relative to within-group variance) suggests that any observed differences between the group means are likely due to random variation, and not a true difference.\n",
        "\n",
        "In summary, the partitioning of variance helps determine whether the variability between groups is large enough relative to the variability within groups to justify concluding that the group means are different. The F-statistic quantifies this by comparing the two types of variance.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "dd7vmZwS5IeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**"
      ],
      "metadata": {
        "id": "Q6qs7xEt56Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Key Differences Between Classical (Frequentist) and Bayesian Approaches to ANOVA:\n",
        "\n",
        "1. **Handling Uncertainty**:\n",
        "   - **Frequentist**: Uncertainty is quantified using **p-values** and **confidence intervals** based on sampling distributions. The true parameters are considered fixed but unknown.\n",
        "   - **Bayesian**: Uncertainty is represented as **probability distributions** over parameters (posterior distributions), which update as more data is observed. Parameters are treated as random variables.\n",
        "\n",
        "2. **Parameter Estimation**:\n",
        "   - **Frequentist**: Estimates are **point estimates** (e.g., sample means), with uncertainty reflected in confidence intervals.\n",
        "   - **Bayesian**: Parameters have **probability distributions** (posterior), reflecting uncertainty about parameter values, and providing a range of plausible values rather than just a single estimate.\n",
        "\n",
        "3. **Hypothesis Testing**:\n",
        "   - **Frequentist**: Hypothesis testing relies on a **null hypothesis**, using **p-values** to decide whether to reject it. Decisions are binary (reject or fail to reject).\n",
        "   - **Bayesian**: Hypothesis testing is based on **posterior probabilities** and **Bayes factors**, comparing the likelihood of different hypotheses or models. It provides a measure of evidence for each hypothesis.\n",
        "\n",
        "### Summary:\n",
        "- **Frequentist**: Focuses on testing specific hypotheses and estimating parameters with point estimates and confidence intervals, using p-values for decision-making.\n",
        "- **Bayesian**: Provides a more flexible approach with probability distributions over parameters, incorporating prior knowledge and offering probabilistic interpretation of results.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FaQPj3lR56gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Question: You have two sets of data representing the incomes of two different professions:**\n",
        "\n",
        "**Profession A:** [48, 52, 55, 60, 62]\n",
        "\n",
        "**Profession B:** [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "**Task: Use Python to calculate the F-statistic and p-value for the given data.**\n",
        "\n",
        "**Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison**"
      ],
      "metadata": {
        "id": "Pffm7qxj6XNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Traq-nO011yZ",
        "outputId": "72c1f5e6-7953-4034-861b-901c6a7fa137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of Profession A: 32.8\n",
            "Variance of Profession B: 15.7\n",
            "F-statistic: 2.089171974522293\n",
            "P-value: 0.49304859900533904\n",
            "Fail to reject the null hypothesis: The variances are equal.\n"
          ]
        }
      ],
      "source": [
        "# We can use Python to calculate the F-statistic and p-value for this test using the SciPy library.\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for the two professions\n",
        "profession_A = [48, 52, 55, 60, 62]\n",
        "profession_B = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate the sample variances\n",
        "var_A = np.var(profession_A, ddof=1)  # ddof=1 for sample variance\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "\n",
        "# Calculate the F-statistic (larger variance / smaller variance)\n",
        "F_statistic = var_A / var_B if var_A >= var_B else var_B / var_A\n",
        "\n",
        "# Degrees of freedom for each sample\n",
        "df_A = len(profession_A) - 1  # df = n - 1 for sample variance\n",
        "df_B = len(profession_B) - 1\n",
        "\n",
        "# Perform the F-test using the F-distribution\n",
        "p_value = 2 * min(stats.f.cdf(F_statistic, df_A, df_B), 1 - stats.f.cdf(F_statistic, df_A, df_B))\n",
        "\n",
        "# Display the results\n",
        "print(f\"Variance of Profession A: {var_A}\")\n",
        "print(f\"Variance of Profession B: {var_B}\")\n",
        "print(f\"F-statistic: {F_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The variances are significantly different.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The variances are equal.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "1. Data: The incomes for two professions are provided as two lists (profession_A and profession_B).\n",
        "2. Variances: The sample variances for each profession are computed using np.var() with ddof=1 (for sample variance).\n",
        "3. F-statistic: The F-statistic is the ratio of the larger variance to the smaller variance.\n",
        "4. P-value: The stats.f.cdf() function computes the cumulative distribution function (CDF) for the 5. 5. 5 F-distribution, and we calculate the p-value using both tails of the distribution.\n",
        "5. Hypothesis Testing: Based on the p-value, we compare it to the significance level (α = 0.05) and decide whether to reject the null hypothesis.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "5l8A_HFw7x9f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data:**\n",
        "\n",
        "* Region A: [160, 162, 165, 158, 164]\n",
        "\n",
        "* Region B: [172, 175, 170, 168, 174]\n",
        "\n",
        "* Region C: [180, 182, 179, 185, 183]\n",
        "\n",
        "* Task: Write Python code to perform the one-way ANOVA\n",
        "and interpret the results.\n",
        "* Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "_9M13-_C8iyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''To perform a one-way ANOVA to test whether there are statistically significant differences in average\n",
        "   heights between the three regions, we can use Python's SciPy library.'''\n",
        "# The one-way ANOVA tests the null hypothesis that the means of the three groups (regions) are equal.\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for the three regions\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "# Display the results\n",
        "print(f\"F-statistic: {f_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "# Conclusion\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference in mean heights.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is no significant difference in mean heights.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhheTAzC7lCj",
        "outputId": "f566ffb8-47d0-4412-ea70-04f4dba903ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "P-value: 2.870664187937026e-07\n",
            "Reject the null hypothesis: There is a significant difference in mean heights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "**1. Data:** We have the height data for three regions: Region A, Region B, and Region C.\n",
        "\n",
        "**2. stats.f_oneway():** This function from the SciPy library performs the one-way ANOVA. It returns the F-statistic and the p-value.\n",
        "\n",
        "**3. Interpretation:**\n",
        "* If p-value < 0.05, we reject the null hypothesis, indicating a significant difference in the means of at least one of the regions.\n",
        "\n",
        "* If p-value ≥ 0.05, we fail to reject the null hypothesis, indicating no significant difference in the means of the regions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "xKOxxKM4-Tev"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1YpVUM8I-wo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}